{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fa540e3",
   "metadata": {},
   "source": [
    "This page.  https://github.com/googlesamples/mediapipe/blob/main/examples/pose_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Pose_Landmarker.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae2119de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /Users/nori/.pyenv/versions/3.9.16/lib/python3.9/site-packages (0.10.9)\n",
      "Requirement already satisfied: absl-py in /Users/nori/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from mediapipe) (1.3.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/nori/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from mediapipe) (22.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/nori/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from mediapipe) (22.12.6)\n",
      "Requirement already satisfied: matplotlib in /Users/nori/.local/lib/python3.9/site-packages (from mediapipe) (3.6.2)\n",
      "Requirement already satisfied: numpy in /Users/nori/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from mediapipe) (1.24.1)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/nori/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from mediapipe) (4.9.0.80)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in /Users/nori/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from mediapipe) (3.19.0)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /Users/nori/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Users/nori/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/nori/.local/lib/python3.9/site-packages (from matplotlib->mediapipe) (1.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/nori/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/nori/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from matplotlib->mediapipe) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/nori/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from matplotlib->mediapipe) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nori/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from matplotlib->mediapipe) (22.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/nori/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from matplotlib->mediapipe) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/nori/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from matplotlib->mediapipe) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/nori/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in /Users/nori/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nori/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f319e8a5",
   "metadata": {},
   "source": [
    "! pip install protobuf==3.19.4\n",
    "\n",
    "protobufがトラブルようなので、以下参考に直す\n",
    "https://qiita.com/johnpapapa/items/e7e06f2480251102d16d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b99024",
   "metadata": {},
   "source": [
    "Then download an off-the-shelf model bundle. Check out the MediaPipe documentation for more information about this model bundle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c91730be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb32aa6",
   "metadata": {},
   "source": [
    "Visualization utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f50f1282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b34ed6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown To better demonstrate the Pose Landmarker API, we have created a set of visualization tools that will be used in this colab. These will draw the landmarks on a detect person, as well as the expected connections between those markers.\n",
    "\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  pose_landmarks_list = detection_result.pose_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected poses to visualize.\n",
    "  for idx in range(len(pose_landmarks_list)):\n",
    "    pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "    # Draw the pose landmarks.\n",
    "    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    pose_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      pose_landmarks_proto,\n",
    "      solutions.pose.POSE_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "  return annotated_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e826c49",
   "metadata": {},
   "source": [
    "Download test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d1712d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image is read.\n",
      "<class 'numpy.ndarray'>\n",
      "(640, 960, 3)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "#!wget -q -O image.jpg https://cdn.pixabay.com/photo/2019/03/12/20/39/girl-4051811_960_720.jpg\n",
    "\n",
    "import cv2\n",
    "#from google.colab.patches import cv2_imshow\n",
    "\n",
    "#img = cv2.imread(\"/Users/nori/documents/github/encount-light/python/mediapipe/image.jpg\")\n",
    "img = cv2.imread(\"image.jpg\")\n",
    "\n",
    "if img is None:\n",
    "    print('Image is not read.')\n",
    "else:\n",
    "    print('Image is read.')\n",
    "# Image is read.\n",
    "\n",
    "\n",
    "#cv2_imshow(img)\n",
    "print(type(img))\n",
    "# <class 'numpy.ndarray'>\n",
    "\n",
    "print(img.shape)\n",
    "# (225, 400, 3)\n",
    "\n",
    "print(img.dtype)\n",
    "# uint8\n",
    "\n",
    "\n",
    "#繰り返しのためのwhile文\n",
    "while True:\n",
    "    cv2.imshow('image', img)\n",
    "    key =cv2.waitKey(1)\n",
    "    \n",
    "    if key == ord('q'):            #qを押した時の処理\n",
    "        cv2.waitKey(1)\n",
    "        cv2.destroyAllWindows()  \n",
    "        #cap.release()\n",
    "        cv2.waitKey(1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a4a0971",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#繰り返しのためのwhile文\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#カメラからの画像取得\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#カメラの画像の出力\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcamera\u001b[39m\u001b[38;5;124m'\u001b[39m , frame)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "#カメラの設定　デバイスIDは0\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#繰り返しのためのwhile文\n",
    "while True:\n",
    "    #カメラからの画像取得\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    #カメラの画像の出力\n",
    "    cv2.imshow('camera' , frame)\n",
    "\n",
    "    #繰り返し分から抜けるためのif文\n",
    "    key =cv2.waitKey(10)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "#メモリを解放して終了するためのコマンド\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401e62f6",
   "metadata": {},
   "source": [
    "Running inference and visualizing the results\n",
    "The final step is to run pose landmark detection on your selected image. This involves creating your PoseLandmarker object, loading your image, running detection, and finally, the optional step of displaying the image with visualizations.\n",
    "\n",
    "Check out the MediaPipe documentation to learn more about configuration options that this solution supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c4f935f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input image must contain three channel bgr data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m detection_result \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39mdetect(image)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# STEP 5: Process the detection result. In this case, visualize it.\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m annotated_image \u001b[38;5;241m=\u001b[39m \u001b[43mdraw_landmarks_on_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy_view\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetection_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#cv2.imshow('image',cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#繰り返しのためのwhile文\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m, in \u001b[0;36mdraw_landmarks_on_image\u001b[0;34m(rgb_image, detection_result)\u001b[0m\n\u001b[1;32m     24\u001b[0m   pose_landmarks_proto \u001b[38;5;241m=\u001b[39m landmark_pb2\u001b[38;5;241m.\u001b[39mNormalizedLandmarkList()\n\u001b[1;32m     25\u001b[0m   pose_landmarks_proto\u001b[38;5;241m.\u001b[39mlandmark\u001b[38;5;241m.\u001b[39mextend([\n\u001b[1;32m     26\u001b[0m     landmark_pb2\u001b[38;5;241m.\u001b[39mNormalizedLandmark(x\u001b[38;5;241m=\u001b[39mlandmark\u001b[38;5;241m.\u001b[39mx, y\u001b[38;5;241m=\u001b[39mlandmark\u001b[38;5;241m.\u001b[39my, z\u001b[38;5;241m=\u001b[39mlandmark\u001b[38;5;241m.\u001b[39mz) \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m pose_landmarks\n\u001b[1;32m     27\u001b[0m   ])\n\u001b[0;32m---> 28\u001b[0m   \u001b[43msolutions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrawing_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_landmarks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mannotated_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpose_landmarks_proto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43msolutions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpose\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOSE_CONNECTIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43msolutions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrawing_styles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_pose_landmarks_style\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m annotated_image\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/mediapipe/python/solutions/drawing_utils.py:154\u001b[0m, in \u001b[0;36mdraw_landmarks\u001b[0;34m(image, landmark_list, connections, landmark_drawing_spec, connection_drawing_spec)\u001b[0m\n\u001b[1;32m    152\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m _BGR_CHANNELS:\n\u001b[0;32m--> 154\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput image must contain three channel bgr data.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    155\u001b[0m image_rows, image_cols, _ \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    156\u001b[0m idx_to_coordinates \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mValueError\u001b[0m: Input image must contain three channel bgr data."
     ]
    }
   ],
   "source": [
    "# STEP 1: Import the necessary modules.\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "import cv2\n",
    "\n",
    "#@markdown To better demonstrate the Pose Landmarker API, we have created a set of visualization tools that will be used in this colab. These will draw the landmarks on a detect person, as well as the expected connections between those markers.\n",
    "\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  pose_landmarks_list = detection_result.pose_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected poses to visualize.\n",
    "  for idx in range(len(pose_landmarks_list)):\n",
    "    pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "    # Draw the pose landmarks.\n",
    "    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    pose_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      pose_landmarks_proto,\n",
    "      solutions.pose.POSE_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "  return annotated_image\n",
    "\n",
    "\n",
    "# STEP 2: Create an PoseLandmarker object.\n",
    "base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    output_segmentation_masks=True)\n",
    "detector = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "# STEP 3: Load the input image.\n",
    "image = mp.Image.create_from_file(\"two.jpg\")\n",
    "\n",
    "# STEP 4: Detect pose landmarks from the input image.\n",
    "detection_result = detector.detect(image)\n",
    "\n",
    "# STEP 5: Process the detection result. In this case, visualize it.\n",
    "annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "#cv2.imshow('image',cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "#繰り返しのためのwhile文\n",
    "while True:\n",
    "    cv2.imshow('image',cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "    key =cv2.waitKey(1)\n",
    "    \n",
    "    if key == ord('q'):            #qを押した時の処理\n",
    "        cv2.waitKey(1)\n",
    "        cv2.destroyAllWindows()  \n",
    "        #cap.release()\n",
    "        cv2.waitKey(1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca52e4",
   "metadata": {},
   "source": [
    "Visualize the pose segmentation mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76e36c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_mask = detection_result.segmentation_masks[0].numpy_view()\n",
    "visualized_mask = np.repeat(segmentation_mask[:, :, np.newaxis], 3, axis=2) * 255\n",
    "#cv2.imshow('mask',visualized_mask)\n",
    "\n",
    "#繰り返しのためのwhile文\n",
    "while True:\n",
    "    cv2.imshow('mask',visualized_mask)\n",
    "    key =cv2.waitKey(1)\n",
    "    \n",
    "    if key == ord('q'):            #qを押した時の処理\n",
    "        cv2.waitKey(1)\n",
    "        cv2.destroyAllWindows()  \n",
    "        #cap.release()\n",
    "        cv2.waitKey(1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7536ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
