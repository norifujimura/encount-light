{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759bf55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "def draw_bodies(image: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    if DETECTION_RESULT:\n",
    "    # Draw landmarks.\n",
    "        for pose_landmarks in DETECTION_RESULT.pose_landmarks:\n",
    "            # Draw the pose landmarks.\n",
    "            pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "            pose_landmarks_proto.landmark.extend([\n",
    "                landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y,\n",
    "                                                z=landmark.z) for landmark\n",
    "                in pose_landmarks\n",
    "            ])\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                pose_landmarks_proto,\n",
    "                mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "    return image\n",
    "\n",
    "def draw_bodies_2(image: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    if DETECTION_RESULT:\n",
    "    # Draw landmarks.\n",
    "        for person in DETECTION_RESULT.pose_landmarks:\n",
    "            # Draw the pose landmarks.\n",
    "            for landmark in person:\n",
    "                 #print(type(landmark))\n",
    "                pos = (int(landmark.x*width),int(landmark.y*height))\n",
    "                # landmark: https://developers.google.com/mediapipe/api/solutions/java/com/google/mediapipe/tasks/components/containers/Landmark\n",
    "                cv2.circle(image,pos, 5, (0,0,255), thickness=-1)\n",
    "                #print(landmark.x)\n",
    "    return image\n",
    "\n",
    "\n",
    "# draw in x-z plane\n",
    "\n",
    "def draw_bodies_world(image: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    if DETECTION_RESULT:\n",
    "        ratio = 100\n",
    "    # Draw landmarks.\n",
    "        for person in DETECTION_RESULT.pose_world_landmarks:\n",
    "            # Draw the pose landmarks.\n",
    "            for landmark in person:\n",
    "                 #print(type(landmark))\n",
    "                pos = (int(landmark.x*ratio),int(landmark.y*ratio))\n",
    "                # landmark: https://developers.google.com/mediapipe/api/solutions/java/com/google/mediapipe/tasks/components/containers/Landmark\n",
    "                cv2.circle(image,pos, 5, (255,0,255), thickness=-1)\n",
    "                #print(landmark.x)\n",
    "    return image\n",
    "\n",
    "def draw_segments(image: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    if (output_segmentation_masks and DETECTION_RESULT):\n",
    "        if DETECTION_RESULT.segmentation_masks is not None:\n",
    "            segmentation_mask = DETECTION_RESULT.segmentation_masks[0].numpy_view()\n",
    "            mask_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "            mask_image[:] = mask_color\n",
    "            condition = np.stack((segmentation_mask,) * 3, axis=-1) > 0.1\n",
    "            visualized_mask = np.where(condition, mask_image, image)\n",
    "            image= cv2.addWeighted(image, overlay_alpha,\n",
    "                                            visualized_mask, overlay_alpha,\n",
    "                                            0)\n",
    "    return image\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Global variables to calculate FPS\n",
    "#COUNTER, FPS = 0, 0\n",
    "#START_TIME = time.time()\n",
    "DETECTION_RESULT = None\n",
    "\n",
    "\"\"\"Continuously run inference on images acquired from the camera.\n",
    "\n",
    "Args:\n",
    "  model: Name of the pose landmarker model bundle.\n",
    "  num_poses: Max number of poses that can be detected by the landmarker.\n",
    "  min_pose_detection_confidence: The minimum confidence score for pose\n",
    "    detection to be considered successful.\n",
    "  min_pose_presence_confidence: The minimum confidence score of pose\n",
    "    presence score in the pose landmark detection.\n",
    "  min_tracking_confidence: The minimum confidence score for the pose\n",
    "    tracking to be considered successful.\n",
    "  output_segmentation_masks: Choose whether to visualize the segmentation\n",
    "    mask or not.\n",
    "  camera_id: The camera id to be passed to OpenCV.\n",
    "  width: The width of the frame captured from the camera.\n",
    "  height: The height of the frame captured from the camera.\n",
    "\"\"\"\n",
    "    \n",
    "camera_id = 1 # 1 for webcam on Mac\n",
    "width = 1280 #1280 for webcam for mac\n",
    "height= 720 # 720 for Webcam for mac\n",
    "\n",
    "model = 'pose_landmarker_lite.task'\n",
    "num_poses = 3\n",
    "min_pose_detection_confidence = 0.5\n",
    "min_pose_presence_confidence = 0.5\n",
    "min_tracking_confidence = 0.5\n",
    "output_segmentation_masks = False\n",
    "fps = 0\n",
    "\n",
    "\n",
    "# Start capturing video input from the camera\n",
    "cap = cv2.VideoCapture(camera_id)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "image = np.zeros((height, width,3), np.uint8)\n",
    "cv2.putText(image, \"hello\", (10, 30),\n",
    "               cv2.FONT_HERSHEY_PLAIN, 1.5,\n",
    "               (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "cv2.startWindowThread()\n",
    "cv2.imshow(model, image)\n",
    "\n",
    "'''\n",
    "# Visualization parameters\n",
    "row_size = 50  # pixels\n",
    "left_margin = 24  # pixels\n",
    "text_color = (0, 0, 0)  # black\n",
    "font_size = 1\n",
    "font_thickness = 1\n",
    "fps_avg_frame_count = 10\n",
    "overlay_alpha = 0.5\n",
    "mask_color = (100, 100, 0)  # cyan\n",
    "'''\n",
    "\n",
    "def callback_result(result: vision.PoseLandmarkerResult,\n",
    "                unused_output_image: mp.Image, timestamp_ms: int):\n",
    "    global fps,DETECTION_RESULT\n",
    "    DETECTION_RESULT = result\n",
    "    '''\n",
    "    # Calculate the FPS\n",
    "    if COUNTER % fps_avg_frame_count == 0:\n",
    "        FPS = fps_avg_frame_count / (time.time() - START_TIME)\n",
    "        START_TIME = time.time()\n",
    "\n",
    "    DETECTION_RESULT = result\n",
    "    COUNTER += 1\n",
    "    '''\n",
    "    \n",
    "# Initialize the pose landmarker model\n",
    "base_options = python.BaseOptions(model_asset_path=model)\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    running_mode=vision.RunningMode.LIVE_STREAM,\n",
    "    num_poses=num_poses,\n",
    "    min_pose_detection_confidence=min_pose_detection_confidence,\n",
    "    min_pose_presence_confidence=min_pose_presence_confidence,\n",
    "    min_tracking_confidence=min_tracking_confidence,\n",
    "    output_segmentation_masks=output_segmentation_masks,\n",
    "    result_callback=callback_result)\n",
    "detector = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Continuously capture images from the camera and run inference\n",
    "while cap.isOpened():\n",
    "    end_time = time.time()\n",
    "    time_diff = end_time - start_time #sec\n",
    "    fps = 1.0 / time_diff\n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        sys.exit(\n",
    "            'ERROR: Unable to read from webcam. Please verify your webcam settings.'\n",
    "        )\n",
    "\n",
    "    image = cv2.flip(image, 1)\n",
    "\n",
    "    # Convert the image from BGR to RGB as required by the TFLite model.\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_image)\n",
    "\n",
    "    # Run pose landmarker using the model.\n",
    "    detector.detect_async(mp_image, time.time_ns() // 1_000_000)\n",
    "\n",
    "    current_frame = image\n",
    "    #current_frame = draw_bodies(current_frame)\n",
    "    current_frame = draw_bodies_2(current_frame)\n",
    "    current_frame = draw_bodies_world(current_frame)\n",
    "    current_frame = draw_segments(current_frame)\n",
    "    \n",
    "    # Show the FPS\n",
    "    #fps_text = 'FPS = ' + str(int(fps))\n",
    "    fps_text = 'FPS = {:.1f}'.format(fps)\n",
    "    text_location = (10,40)\n",
    "    cv2.putText(current_frame, fps_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
    "                2, (255,255,255), 1)\n",
    "\n",
    "    cv2.imshow('pose_landmarker', current_frame)\n",
    "\n",
    "    time.sleep(0.1)\n",
    "    key = cv2.waitKey(1)\n",
    "\n",
    "    if key == ord('q'):            #qを押した時の処理\n",
    "        cv2.waitKey(1)\n",
    "        cv2.destroyAllWindows()  \n",
    "        cap.release()\n",
    "        cv2.waitKey(1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52316ac",
   "metadata": {},
   "source": [
    "これを見るとデータの仕様がわかる　https://developers.google.com/mediapipe/solutions/vision/pose_landmarker/python\n",
    "\n",
    "The output contains the following world coordinates (WorldLandmarks):\n",
    "\n",
    "x, y, and z: Real-world 3-dimensional coordinates in meters, with the midpoint of the hips as the origin.\n",
    "\n",
    "visibility: The likelihood of the landmark being visible within the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bb9d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad9dc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
