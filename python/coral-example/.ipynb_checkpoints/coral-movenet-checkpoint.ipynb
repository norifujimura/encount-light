{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78b4fa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33184516 0.5776564  0.49981618]\n",
      " [0.31955463 0.589947   0.6350124 ]\n",
      " [0.3113609  0.56536585 0.70056206]\n",
      " [0.31955463 0.5776564  0.43016967]\n",
      " [0.31955463 0.5080099  0.29907033]\n",
      " [0.42197597 0.5776564  0.6350124 ]\n",
      " [0.41378227 0.4178791  0.49981618]\n",
      " [0.50391304 0.7128526  0.19664899]\n",
      " [0.5121068  0.5407847  0.49981618]\n",
      " [0.42197597 0.6964652  0.49981618]\n",
      " [0.42197597 0.6677872  0.15568045]\n",
      " [0.59814066 0.38920113 0.75382113]\n",
      " [0.6145281  0.2867798  0.43016967]\n",
      " [0.6595935  0.5858501  0.6350124 ]\n",
      " [0.6759809  0.42197597 0.75382113]\n",
      " [0.8480488  0.5121068  0.6350124 ]\n",
      " [0.88901734 0.3400389  0.70056206]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z6/9647yzcd6s52_n2gsnrlv9z00000gn/T/ipykernel_31438/4118801588.py:27: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  resized_img = img.resize(common.input_size(interpreter), Image.ANTIALIAS)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m     img\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone. Results saved at\u001b[39m\u001b[38;5;124m'\u001b[39m, args\u001b[38;5;241m.\u001b[39moutput)\n\u001b[0;32m---> 46\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     draw\u001b[38;5;241m.\u001b[39mellipse(\n\u001b[1;32m     38\u001b[0m         xy\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     39\u001b[0m             pose[i][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m width \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m, pose[i][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m height \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     40\u001b[0m             pose[i][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m width \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m, pose[i][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m height \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     41\u001b[0m         ],\n\u001b[1;32m     42\u001b[0m         fill\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     43\u001b[0m img\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone. Results saved at\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39moutput)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from pycoral.adapters import common\n",
    "from pycoral.utils.edgetpu import make_interpreter\n",
    "\n",
    "_NUM_KEYPOINTS = 17\n",
    "\n",
    "def run():\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\n",
    "      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\n",
    "      '-m', '--model', required=True, help='File path of .tflite file.')\n",
    "    parser.add_argument(\n",
    "      '-i', '--input', required=True, help='Image to be classified.')\n",
    "    parser.add_argument(\n",
    "      '--output',\n",
    "      default='movenet_result.jpg',\n",
    "      help='File path of the output image.')\n",
    "    args = parser.parse_args()\n",
    "    '''\n",
    "    # /Users/nori/Documents/GitHub/encount-light/python/coral-example/movenet_single_pose_lightning_ptq_edgetpu.tflite\n",
    "    interpreter = make_interpreter('/Users/nori/Documents/GitHub/encount-light/python/coral-example/movenet_single_pose_lightning_ptq_edgetpu.tflite')\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    img = Image.open('/Users/nori/Documents/GitHub/encount-light/python/coral-example/squat.bmp')\n",
    "    resized_img = img.resize(common.input_size(interpreter), Image.ANTIALIAS)\n",
    "    common.set_input(interpreter, resized_img)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    pose = common.output_tensor(interpreter, 0).copy().reshape(_NUM_KEYPOINTS, 3)\n",
    "    print(pose)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    width, height = img.size\n",
    "    for i in range(0, _NUM_KEYPOINTS):\n",
    "        draw.ellipse(\n",
    "            xy=[\n",
    "                pose[i][1] * width - 2, pose[i][0] * height - 2,\n",
    "                pose[i][1] * width + 2, pose[i][0] * height + 2\n",
    "            ],\n",
    "            fill=(255, 0, 0))\n",
    "    img.save('output.jpg')\n",
    "    print('Done. Results saved at', args.output)\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be59a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "single personの検出ができた"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6efb1a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import serial\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import utils\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "# from PIL import Image\n",
    "# from PIL import ImageDraw\n",
    "from pycoral.adapters import common\n",
    "from pycoral.utils.edgetpu import make_interpreter\n",
    "\n",
    "_NUM_KEYPOINTS = 17\n",
    "\n",
    "width = 1024\n",
    "height = 720\n",
    "camera_id = 0\n",
    "\n",
    "\n",
    "# Start capturing video input from the camera\n",
    "cap = cv2.VideoCapture(camera_id)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "image = np.zeros((height, width,3), np.uint8)\n",
    "cv2.putText(image, \"hello\", (10, 30),\n",
    "               cv2.FONT_HERSHEY_PLAIN, 1.5,\n",
    "               (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "cv2.startWindowThread()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#coral\n",
    "# /Users/nori/Documents/GitHub/encount-light/python/coral-example/movenet_single_pose_lightning_ptq_edgetpu.tflite\n",
    "interpreter = make_interpreter('/Users/nori/Documents/GitHub/encount-light/python/coral-example/movenet_single_pose_thunder_ptq_edgetpu.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "while cap.isOpened():\n",
    "    global start_time, interpreter\n",
    "    \n",
    "    #calc fps\n",
    "    end_time = time.time()\n",
    "    \n",
    "    time_diff = end_time - start_time #sec\n",
    "    fps = 1.0 / time_diff\n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #capture\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      sys.exit(\n",
    "          'ERROR: Unable to read from webcam. Please verify your webcam settings.'\n",
    "      )\n",
    "\n",
    "    image = cv2.flip(image, 1)\n",
    "    # persons = pose_detector.detect(image)\n",
    "    \n",
    "    #inference\n",
    "    # cv2.resize(img, Size,fx=0,fy=0,interpolation=INTER_LINEAR)\n",
    "    resized_img = cv2.resize(image,common.input_size(interpreter), interpolation = cv2.INTER_AREA)\n",
    "    common.set_input(interpreter, resized_img)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    pose = common.output_tensor(interpreter, 0).copy().reshape(_NUM_KEYPOINTS, 3)\n",
    "    # print(pose)\n",
    "\n",
    "    # Draw keypoints and edges on input image\n",
    "    \n",
    "    for i in range(0, _NUM_KEYPOINTS):\n",
    "        x = int(pose[i][1] * width)\n",
    "        y = int(pose[i][0] * height)\n",
    "        color = (255, 255, 255)\n",
    "        diameter = 2\n",
    "        if i ==0:\n",
    "            color = (255, 0,0)\n",
    "            diameter = 5\n",
    "        if i ==1 or i ==2:\n",
    "            color = (0, 255,0)\n",
    "            diameter = 5\n",
    "        if i ==3 or i ==4:\n",
    "            color = (0, 0,255)\n",
    "            diameter = 5\n",
    "        if i ==5 or i ==6:\n",
    "            color = (255, 0,255)\n",
    "            diameter = 7\n",
    "        cv2.circle(image, (x,y), diameter, color, thickness=-1)\n",
    "\n",
    "    # Show the FPS\n",
    "    fps_text = 'FPS = ' + str(int(fps))\n",
    "    text_location = (10,10)\n",
    "    cv2.putText(image, fps_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
    "                1, (255,255,255), 1)\n",
    "    \n",
    "    cv2.imshow(\"Coral single detect\", image)\n",
    "    time.sleep(0.05)\n",
    "    key = cv2.waitKey(1)\n",
    "\n",
    "    if key == ord('q'):            #qを押した時の処理\n",
    "        cv2.waitKey(1)\n",
    "        cv2.destroyAllWindows()  \n",
    "        cap.release()\n",
    "        cv2.waitKey(1)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7396a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "課題：singlepersonのboundaryboxを塗りつぶして人数分だけ繰り返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451da07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import serial\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import utils\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "# from PIL import Image\n",
    "# from PIL import ImageDraw\n",
    "from pycoral.adapters import common\n",
    "from pycoral.utils.edgetpu import make_interpreter\n",
    "\n",
    "_NUM_KEYPOINTS = 17\n",
    "\n",
    "width = 1024\n",
    "height = 720\n",
    "camera_id = 0\n",
    "\n",
    "\n",
    "# Start capturing video input from the camera\n",
    "cap = cv2.VideoCapture(camera_id)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# image = np.zeros((height, width,3), np.uint8)\n",
    "\n",
    "cv2.startWindowThread()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#coral\n",
    "# /Users/nori/Documents/GitHub/encount-light/python/coral-example/movenet_single_pose_lightning_ptq_edgetpu.tflite\n",
    "interpreter = make_interpreter('/Users/nori/Documents/GitHub/encount-light/python/coral-example/movenet_single_pose_thunder_ptq_edgetpu.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "def draw_points(\n",
    "    image: np.ndarray,\n",
    "    points:np.ndarray\n",
    ") -> np.ndarray:\n",
    "    # Draw keypoints and edges on input image\n",
    "\n",
    "    for i in range(0, _NUM_KEYPOINTS):\n",
    "        x = int(points[i][1] * width)\n",
    "        y = int(points[i][0] * height)\n",
    "        color = (255, 255, 255)\n",
    "        diameter = 2\n",
    "        if i ==0:\n",
    "            color = (255, 0,0)\n",
    "            diameter = 5\n",
    "        if i ==1 or i ==2:\n",
    "            color = (0, 255,0)\n",
    "            diameter = 5\n",
    "        if i ==3 or i ==4:\n",
    "            color = (0, 0,255)\n",
    "            diameter = 5\n",
    "        if i ==5 or i ==6:\n",
    "            color = (255, 0,255)\n",
    "            diameter = 7\n",
    "        cv2.circle(image, (x,y), diameter, color, thickness=-1)\n",
    "    return image\n",
    "\n",
    "def bounding_box(\n",
    "    image: np.ndarray,\n",
    "    points:np.ndarray\n",
    ")-> np.ndarray:\n",
    "    minx = 10000\n",
    "    miny = 10000\n",
    "    maxx = 0\n",
    "    maxy = 0\n",
    "    \n",
    "    for point in points:\n",
    "        if point[1]<minx:\n",
    "            minx = point[1]\n",
    "        if point[1]>maxx:\n",
    "            maxx = point[1]\n",
    "        if point[0]<miny:\n",
    "            miny = point[0]\n",
    "        if point[0]>maxy:\n",
    "            maxy = point[0]\n",
    "            \n",
    "    x = int(minx * width)\n",
    "    y = int(miny * height)\n",
    "    xtwo = int(maxx * width)\n",
    "    ytwo = int(maxy * height)\n",
    "\n",
    "    cv2.rectangle(image, (x,y), (xtwo,ytwo), (0,0, 0), thickness=-1)\n",
    "            \n",
    "    return image\n",
    "\n",
    "def bounding_circle(\n",
    "    image: np.ndarray,\n",
    "    points:np.ndarray\n",
    ")-> np.ndarray:\n",
    "    accum_x= 0\n",
    "    accum_y= 0\n",
    "    \n",
    "    for point in points:\n",
    "        accum_x+=point[1]\n",
    "        accum_y+=point[0]\n",
    "        \n",
    "    center = (accum_x/len(points),accum_y/len(points));\n",
    "    \n",
    "    r = 0;\n",
    "    for point in points:\n",
    "        distance = math.sqrt((point[1] - center[0])**2 + (point[0] - center[1])**2)\n",
    "        if(r<distance):\n",
    "            r= distance\n",
    "\n",
    "    x = int(center[0] * width)\n",
    "    y = int(center[1] * height)\n",
    "    r = r* width\n",
    "    cv2.circle(image, (x,y), int(r), (0,0,0), thickness=-1)\n",
    "    return image\n",
    "\n",
    "def inference(\n",
    "    img: np.ndarray\n",
    ")-> np.ndarray:\n",
    "    #inference\n",
    "    resized_img = cv2.resize(img,common.input_size(interpreter), interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    common.set_input(interpreter, resized_img)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    pose = common.output_tensor(interpreter, 0).copy().reshape(_NUM_KEYPOINTS, 3)\n",
    "    \n",
    "    if len(pose)==0:\n",
    "        return img\n",
    "    # print(pose)\n",
    "    #print(type(pose))\n",
    "    \n",
    "    array = []\n",
    "    array.append(pose[0])\n",
    "    array.append(pose[1])\n",
    "    array.append(pose[2])\n",
    "    \n",
    "    array.append(pose[3])\n",
    "    array.append(pose[4])\n",
    "    array.append(pose[5])\n",
    "    array.append(pose[6])\n",
    "    img = draw_points(img,pose)\n",
    "    img = bounding_circle(img,array)\n",
    "    return img\n",
    "        \n",
    "while cap.isOpened():\n",
    "    global start_time, interpreter\n",
    "    \n",
    "    #calc fps\n",
    "    end_time = time.time()\n",
    "    \n",
    "    time_diff = end_time - start_time #sec\n",
    "    fps = 1.0 / time_diff\n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #capture\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      sys.exit(\n",
    "          'ERROR: Unable to read from webcam. Please verify your webcam settings.'\n",
    "      )\n",
    "\n",
    "    image = cv2.flip(image, 1)\n",
    "    \n",
    "    image_two = inference(image)\n",
    "    image_three = inference(image_two)\n",
    "    \n",
    "    merge = np.hstack((image_two, image_three))\n",
    "\n",
    "    # Show the FPS\n",
    "    fps_text = 'FPS = ' + str(int(fps))\n",
    "    text_location = (10,10)\n",
    "    cv2.putText(image_two, fps_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
    "                1, (255,255,255), 1)\n",
    "    \n",
    "    cv2.imshow(\"Coral single detect\", merge )\n",
    "    time.sleep(0.1)\n",
    "    key = cv2.waitKey(1)\n",
    "\n",
    "    if key == ord('q'):            #qを押した時の処理\n",
    "        cv2.waitKey(1)\n",
    "        cv2.destroyAllWindows()  \n",
    "        cap.release()\n",
    "        cv2.waitKey(1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec64b91b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
