{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a42dbae0",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/lite/examples/pose_estimation/overview?hl=ja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c2ba3a",
   "metadata": {},
   "source": [
    "https://blog.tensorflow.org/2021/08/pose-estimation-and-classification-on-edge-devices-with-MoveNet-and-TensorFlow-Lite.html?hl=ja&_gl=1*xgz6jk*_ga*MTYwMDI2MzYzMC4xNzA0MTcwODE4*_ga_W0YLR4190T*MTcwNjUxOTU5NC41LjEuMTcwNjUyMDE0OC4wLjAuMA.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b0bd12",
   "metadata": {},
   "source": [
    "TFlite MOvenet RasPi sample code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9bcb01",
   "metadata": {},
   "source": [
    "https://github.com/tensorflow/examples/tree/master/lite/examples/pose_estimation/raspberry_pi#run-the-pose-classification-sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b2cea",
   "metadata": {},
   "source": [
    "Install dependencies. \n",
    "Run this script to install the Python dependencies, and download the TFLite models. sh setup.sh\n",
    "\n",
    "for the error following:\n",
    "ERROR: Could not find a version that satisfies the requirement tflite-runtime>=2.7.0 (from versions: none)\n",
    "https://temcee.hatenablog.com/entry/tensorflow_install_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a272e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sh setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145fbbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 pose_estimation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081fa5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 pose_estimation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea2c44c",
   "metadata": {},
   "source": [
    "上記部分はmodule not found errorがでていたがカーネル再起動で通った"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e40e32f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main script to run pose classification and pose estimation.\"\"\"\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "\n",
    "import utils\n",
    "from ml import Classifier\n",
    "from ml import Movenet\n",
    "from ml import MoveNetMultiPose\n",
    "from ml import Posenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90983c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myRun(camera_id: int, width: int, height: int) -> None:\n",
    "    model = 'movenet_multipose'\n",
    "    pose_detector = MoveNetMultiPose(model,'bounding_box',512)\n",
    "\n",
    "    # Variables to calculate FPS\n",
    "    counter, fps = 0, 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Start capturing video input from the camera\n",
    "    cap = cv2.VideoCapture(camera_id)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "    # Visualization parameters\n",
    "    row_size = 20  # pixels\n",
    "    left_margin = 24  # pixels\n",
    "    text_color = (0, 0, 255)  # red\n",
    "    font_size = 1\n",
    "    font_thickness = 1\n",
    "    classification_results_to_show = 3\n",
    "    fps_avg_frame_count = 10\n",
    "    keypoint_detection_threshold_for_classifier = 0.1\n",
    "    classifier = None\n",
    "    \n",
    "    #key = cv2.waitKey(0)\n",
    "    #cv2.startWindowThread()\n",
    "\n",
    "    # Continuously capture images from the camera and run inference\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "          sys.exit(\n",
    "              'ERROR: Unable to read from webcam. Please verify your webcam settings.'\n",
    "          )\n",
    "\n",
    "        counter += 1\n",
    "        image = cv2.flip(image, 1)\n",
    "        persons = pose_detector.detect(image)\n",
    "\n",
    "        # Draw keypoints and edges on input image\n",
    "        image = utils.visualize(image, persons)\n",
    "\n",
    "        # Calculate the FPS\n",
    "        if counter % fps_avg_frame_count == 0:\n",
    "          end_time = time.time()\n",
    "          fps = fps_avg_frame_count / (end_time - start_time)\n",
    "          start_time = time.time()\n",
    "\n",
    "        # Show the FPS\n",
    "        fps_text = 'FPS = ' + str(int(fps))\n",
    "        text_location = (left_margin, row_size)\n",
    "        cv2.putText(image, fps_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
    "                    font_size, text_color, font_thickness)\n",
    "\n",
    "        # Stop the program if the q key is pressed.\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "          break\n",
    "        cv2.imshow(model, image)\n",
    "        '''\n",
    "        key = cv2.waitKey(0)\n",
    "\n",
    "        if key == ord('q'):            #qを押した時の処理\n",
    "            cv2.waitKey(1)\n",
    "            cv2.destroyAllWindows()  \n",
    "            cap.release()\n",
    "            cv2.waitKey(1)\n",
    "            '''\n",
    "            \n",
    "    cv2.destroyAllWindows()  \n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "237a7e57",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "ERROR: Unable to read from webcam. Please verify your webcam settings.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m ERROR: Unable to read from webcam. Please verify your webcam settings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nori/.pyenv/versions/3.9.16/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3441: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "myRun(0, 1280, 720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaec357",
   "metadata": {},
   "source": [
    "qを押すとクラッシュしないように閉じる版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43ddc251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import serial\n",
    "import time\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import utils\n",
    "\n",
    "from ml import Classifier\n",
    "from ml import Movenet\n",
    "from ml import MoveNetMultiPose\n",
    "from ml import Posenet\n",
    "\n",
    "model = 'movenet_multipose'\n",
    "pose_detector = MoveNetMultiPose(model,'bounding_box',512)\n",
    "\n",
    "width = 1024\n",
    "height = 720\n",
    "camera_id = 0\n",
    "\n",
    "# Start capturing video input from the camera\n",
    "cap = cv2.VideoCapture(camera_id)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "image = np.zeros((720, 1280,3), np.uint8)\n",
    "cv2.putText(image, \"hello\", (10, 30),\n",
    "               cv2.FONT_HERSHEY_PLAIN, 1.5,\n",
    "               (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "cv2.startWindowThread()\n",
    "cv2.imshow(model, image)\n",
    "\n",
    "# Visualization parameters\n",
    "row_size = 20  # pixels\n",
    "left_margin = 24  # pixels\n",
    "text_color = (0, 0, 255)  # red\n",
    "font_size = 1\n",
    "font_thickness = 1\n",
    "classification_results_to_show = 3\n",
    "fps_avg_frame_count = 10\n",
    "keypoint_detection_threshold_for_classifier = 0.1\n",
    "classifier = None\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while cap.isOpened():\n",
    "    end_time = time.time()\n",
    "    \n",
    "    time_diff = end_time - start_time #sec\n",
    "    fps = 1.0 / time_diff\n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      sys.exit(\n",
    "          'ERROR: Unable to read from webcam. Please verify your webcam settings.'\n",
    "      )\n",
    "\n",
    "    image = cv2.flip(image, 1)\n",
    "    persons = pose_detector.detect(image)\n",
    "\n",
    "    # Draw keypoints and edges on input image\n",
    "    image = utils.visualize(image, persons)\n",
    "    \n",
    "    #canvas = np.zeros((720, 1280,3), np.uint8)\n",
    "\n",
    "    # Show the FPS\n",
    "    fps_text = 'FPS = ' + str(int(fps))\n",
    "    text_location = (left_margin, row_size)\n",
    "    cv2.putText(image, fps_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
    "                font_size, text_color, font_thickness)\n",
    "    time.sleep(0.01)\n",
    "    \n",
    "    cv2.imshow(model, image)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "\n",
    "    if key == ord('q'):            #qを押した時の処理\n",
    "        cv2.waitKey(1)\n",
    "        cv2.destroyAllWindows()  \n",
    "        cap.release()\n",
    "        cv2.waitKey(1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789b7ec8",
   "metadata": {},
   "source": [
    "課題：opencvで画面を出して、配列５m分を用意して表示する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ef80a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 08:37:20.372 python3.9[11972:472091] _TIPropertyValueIsValid called with 4 on nil context!\n",
      "2024-02-08 08:37:20.372 python3.9[11972:472091] imkxpc_getApplicationProperty:reply: called with incorrect property value 4, bailing.\n",
      "2024-02-08 08:37:20.372 python3.9[11972:472091] Text input context does not respond to _valueForTIProperty:\n",
      "2024-02-08 08:37:20.494 python3.9[11972:472091] _TIPropertyValueIsValid called with 4 on nil context!\n",
      "2024-02-08 08:37:20.494 python3.9[11972:472091] imkxpc_getApplicationProperty:reply: called with incorrect property value 4, bailing.\n",
      "2024-02-08 08:37:20.494 python3.9[11972:472091] Text input context does not respond to _valueForTIProperty:\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import serial\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import utils\n",
    "\n",
    "from typing import List, Tuple\n",
    "from data import Person\n",
    "\n",
    "from ml import Classifier\n",
    "from ml import Movenet\n",
    "from ml import MoveNetMultiPose\n",
    "from ml import Posenet\n",
    "\n",
    "class Body:\n",
    "    \n",
    "def process_bodies(\n",
    "    list_persons: List[Person],\n",
    "    keypoint_threshold: float = 0.8,\n",
    "    instance_threshold: float = 0.8,\n",
    ")-> List[Body]:\n",
    "    \n",
    "\n",
    "def draw_bodies(\n",
    "    image: np.ndarray,\n",
    "    list_persons: List[Person],\n",
    "    keypoint_color: Tuple[int, ...] = None,\n",
    "    keypoint_threshold: float = 0.8,\n",
    "    instance_threshold: float = 0.8,\n",
    ") -> np.ndarray:\n",
    "    for person in list_persons:\n",
    "        if person.score < instance_threshold:\n",
    "            continue\n",
    "        keypoints = person.keypoints\n",
    "        bounding_box = person.bounding_box\n",
    "        \n",
    "        # Draw bounding_box with multipose\n",
    "        if bounding_box is not None:\n",
    "            start_point = (bounding_box.start_point[0],int(height/2))\n",
    "            end_point = (bounding_box.end_point[0],int(height/2))\n",
    "            cv2.rectangle(image, start_point, end_point, (255, 255, 255), 2)\n",
    "            # Draw id text when tracker is enabled for MoveNet MultiPose model.\n",
    "            # (id = None when using single pose model or when tracker is None)\n",
    "            if person.id:\n",
    "                id_text = 'id = ' + str(person.id)\n",
    "                cv2.putText(image, id_text, start_point, cv2.FONT_HERSHEY_PLAIN, 1,(0, 0, 255), 1)\n",
    "    return image\n",
    "\n",
    "model = 'movenet_multipose'\n",
    "pose_detector = MoveNetMultiPose(model,'bounding_box',512)\n",
    "\n",
    "width = 1024\n",
    "height = 720\n",
    "camera_id = 0\n",
    "\n",
    "led_length = 150;  #5m 150\n",
    "leds = [0] * led_length\n",
    "\n",
    "# Start capturing video input from the camera\n",
    "cap = cv2.VideoCapture(camera_id)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "image = np.zeros((720, 1280,3), np.uint8)\n",
    "cv2.putText(image, \"hello\", (10, 30),\n",
    "               cv2.FONT_HERSHEY_PLAIN, 1.5,\n",
    "               (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "cv2.startWindowThread()\n",
    "cv2.imshow(model, image)\n",
    "\n",
    "# Visualization parameters\n",
    "row_size = 20  # pixels\n",
    "left_margin = 24  # pixels\n",
    "text_color = (0, 0, 255)  # red\n",
    "font_size = 1\n",
    "font_thickness = 1\n",
    "classification_results_to_show = 3\n",
    "fps_avg_frame_count = 10\n",
    "keypoint_detection_threshold_for_classifier = 0.1\n",
    "classifier = None\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while cap.isOpened():\n",
    "    end_time = time.time()\n",
    "    \n",
    "    time_diff = end_time - start_time #sec\n",
    "    fps = 1.0 / time_diff\n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      sys.exit(\n",
    "          'ERROR: Unable to read from webcam. Please verify your webcam settings.'\n",
    "      )\n",
    "\n",
    "    image = cv2.flip(image, 1)\n",
    "    persons = pose_detector.detect(image)\n",
    "\n",
    "    # Draw keypoints and edges on input image\n",
    "    image = utils.visualize(image, persons,keypoint_threshold=0.2,instance_threshold=0.2)\n",
    "    image = draw_bodies(image, persons,keypoint_threshold=0.2,instance_threshold=0.2)\n",
    "\n",
    "    # Show the FPS\n",
    "    fps_text = 'FPS = ' + str(int(fps))\n",
    "    text_location = (left_margin, row_size)\n",
    "    cv2.putText(image, fps_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
    "                font_size, text_color, font_thickness)\n",
    "    \n",
    "    \n",
    "    cv2.imshow(model, image)\n",
    "    time.sleep(0.01)\n",
    "    key = cv2.waitKey(1)\n",
    "\n",
    "    if key == ord('q'):            #qを押した時の処理\n",
    "        cv2.waitKey(1)\n",
    "        cv2.destroyAllWindows()  \n",
    "        cap.release()\n",
    "        cv2.waitKey(1)\n",
    "        break\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9adbddc",
   "metadata": {},
   "source": [
    "課題：Serialに繋いで、バイト配列を送る"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
