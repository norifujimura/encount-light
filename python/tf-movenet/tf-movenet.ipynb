{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a42dbae0",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/lite/examples/pose_estimation/overview?hl=ja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c2ba3a",
   "metadata": {},
   "source": [
    "https://blog.tensorflow.org/2021/08/pose-estimation-and-classification-on-edge-devices-with-MoveNet-and-TensorFlow-Lite.html?hl=ja&_gl=1*xgz6jk*_ga*MTYwMDI2MzYzMC4xNzA0MTcwODE4*_ga_W0YLR4190T*MTcwNjUxOTU5NC41LjEuMTcwNjUyMDE0OC4wLjAuMA.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b0bd12",
   "metadata": {},
   "source": [
    "TFlite MOvenet RasPi sample code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9bcb01",
   "metadata": {},
   "source": [
    "https://github.com/tensorflow/examples/tree/master/lite/examples/pose_estimation/raspberry_pi#run-the-pose-classification-sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b2cea",
   "metadata": {},
   "source": [
    "Install dependencies. \n",
    "Run this script to install the Python dependencies, and download the TFLite models. sh setup.sh\n",
    "\n",
    "for the error following:\n",
    "ERROR: Could not find a version that satisfies the requirement tflite-runtime>=2.7.0 (from versions: none)\n",
    "https://temcee.hatenablog.com/entry/tensorflow_install_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a272e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sh setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145fbbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 pose_estimation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081fa5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 pose_estimation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea2c44c",
   "metadata": {},
   "source": [
    "上記部分はmodule not found errorがでていたがカーネル再起動で通った"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "989aa255",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main script to run pose classification and pose estimation.\"\"\"\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "\n",
    "import utils\n",
    "from ml import Classifier\n",
    "from ml import Movenet\n",
    "from ml import MoveNetMultiPose\n",
    "from ml import Posenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90983c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myRun(camera_id: int, width: int, height: int) -> None:\n",
    "    model = 'movenet_multipose'\n",
    "    pose_detector = MoveNetMultiPose(model,'bounding_box',512)\n",
    "\n",
    "    # Variables to calculate FPS\n",
    "    counter, fps = 0, 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Start capturing video input from the camera\n",
    "    cap = cv2.VideoCapture(camera_id)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "    # Visualization parameters\n",
    "    row_size = 20  # pixels\n",
    "    left_margin = 24  # pixels\n",
    "    text_color = (0, 0, 255)  # red\n",
    "    font_size = 1\n",
    "    font_thickness = 1\n",
    "    classification_results_to_show = 3\n",
    "    fps_avg_frame_count = 10\n",
    "    keypoint_detection_threshold_for_classifier = 0.1\n",
    "    classifier = None\n",
    "\n",
    "    # Continuously capture images from the camera and run inference\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "          sys.exit(\n",
    "              'ERROR: Unable to read from webcam. Please verify your webcam settings.'\n",
    "          )\n",
    "\n",
    "        counter += 1\n",
    "        image = cv2.flip(image, 1)\n",
    "        persons = pose_detector.detect(image)\n",
    "\n",
    "        # Draw keypoints and edges on input image\n",
    "        image = utils.visualize(image, persons)\n",
    "\n",
    "        # Calculate the FPS\n",
    "        if counter % fps_avg_frame_count == 0:\n",
    "          end_time = time.time()\n",
    "          fps = fps_avg_frame_count / (end_time - start_time)\n",
    "          start_time = time.time()\n",
    "\n",
    "        # Show the FPS\n",
    "        fps_text = 'FPS = ' + str(int(fps))\n",
    "        text_location = (left_margin, row_size)\n",
    "        cv2.putText(image, fps_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
    "                    font_size, text_color, font_thickness)\n",
    "\n",
    "        # Stop the program if the ESC key is pressed.\n",
    "        if cv2.waitKey(1) == 27:\n",
    "          break\n",
    "        cv2.imshow(model, image)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "237a7e57",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmyRun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1280\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m720\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 35\u001b[0m, in \u001b[0;36mmyRun\u001b[0;34m(camera_id, width, height)\u001b[0m\n\u001b[1;32m     33\u001b[0m counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     34\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mflip(image, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m persons \u001b[38;5;241m=\u001b[39m \u001b[43mpose_detector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Draw keypoints and edges on input image\u001b[39;00m\n\u001b[1;32m     38\u001b[0m image \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mvisualize(image, persons)\n",
      "File \u001b[0;32m~/Documents/GitHub/encount-light/python/tf-movenet/ml/movenet_multipose.py:121\u001b[0m, in \u001b[0;36mMoveNetMultiPose.detect\u001b[0;34m(self, input_image, detection_threshold)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Run inference with the MoveNet MultiPose model.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpreter\u001b[38;5;241m.\u001b[39mset_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    120\u001b[0m                              input_tensor\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_type))\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Get the model output\u001b[39;00m\n\u001b[1;32m    124\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpreter\u001b[38;5;241m.\u001b[39mget_tensor(\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/tflite_runtime/interpreter.py:833\u001b[0m, in \u001b[0;36mInterpreter.invoke\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;124;03m\"\"\"Invoke the interpreter.\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \n\u001b[1;32m    823\u001b[0m \u001b[38;5;124;03mBe sure to set the input sizes, allocate tensors and fill values before\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;124;03m  ValueError: When the underlying interpreter fails raise ValueError.\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_safe()\n\u001b[0;32m--> 833\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "myRun(1, 1280, 720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137fdec9",
   "metadata": {},
   "source": [
    "課題：opencvで画面を出して、配列１０m分を用意して表示する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb819100",
   "metadata": {},
   "source": [
    "課題：Serialに繋いで、バイト配列を送る"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
