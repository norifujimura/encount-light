{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "506df9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No tracker will be used as tracker can only be enabled for MoveNet MultiPose model.\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Main script to run pose classification and pose estimation.\"\"\"\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "from ml import Classifier\n",
    "from ml import Movenet\n",
    "from ml import MoveNetMultiPose\n",
    "from ml import Posenet\n",
    "import utils\n",
    "\n",
    "\n",
    "def run(estimation_model: str, tracker_type: str, classification_model: str,\n",
    "        label_file: str, camera_id: int, width: int, height: int) -> None:\n",
    "  \"\"\"Continuously run inference on images acquired from the camera.\n",
    "\n",
    "  Args:\n",
    "    estimation_model: Name of the TFLite pose estimation model.\n",
    "    tracker_type: Type of Tracker('keypoint' or 'bounding_box').\n",
    "    classification_model: Name of the TFLite pose classification model.\n",
    "      (Optional)\n",
    "    label_file: Path to the label file for the pose classification model. Class\n",
    "      names are listed one name per line, in the same order as in the\n",
    "      classification model output. See an example in the yoga_labels.txt file.\n",
    "    camera_id: The camera id to be passed to OpenCV.\n",
    "    width: The width of the frame captured from the camera.\n",
    "    height: The height of the frame captured from the camera.\n",
    "  \"\"\"\n",
    "\n",
    "  # Notify users that tracker is only enabled for MoveNet MultiPose model.\n",
    "  if tracker_type and (estimation_model != 'movenet_multipose'):\n",
    "    logging.warning(\n",
    "        'No tracker will be used as tracker can only be enabled for '\n",
    "        'MoveNet MultiPose model.')\n",
    "\n",
    "  # Initialize the pose estimator selected.\n",
    "  if estimation_model in ['movenet_lightning', 'movenet_thunder']:\n",
    "    pose_detector = Movenet(estimation_model)\n",
    "  elif estimation_model == 'posenet':\n",
    "    pose_detector = Posenet(estimation_model)\n",
    "  elif estimation_model == 'movenet_multipose':\n",
    "    pose_detector = MoveNetMultiPose(estimation_model, tracker_type)\n",
    "  else:\n",
    "    sys.exit('ERROR: Model is not supported.')\n",
    "\n",
    "  # Variables to calculate FPS\n",
    "  counter, fps = 0, 0\n",
    "  start_time = time.time()\n",
    "\n",
    "  # Start capturing video input from the camera\n",
    "  cap = cv2.VideoCapture(camera_id)\n",
    "  cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "  cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "  # Visualization parameters\n",
    "  row_size = 20  # pixels\n",
    "  left_margin = 24  # pixels\n",
    "  text_color = (0, 0, 255)  # red\n",
    "  font_size = 1\n",
    "  font_thickness = 1\n",
    "  classification_results_to_show = 3\n",
    "  fps_avg_frame_count = 10\n",
    "  keypoint_detection_threshold_for_classifier = 0.1\n",
    "  classifier = None\n",
    "\n",
    "  # Initialize the classification model\n",
    "  if classification_model:\n",
    "    classifier = Classifier(classification_model, label_file)\n",
    "    classification_results_to_show = min(classification_results_to_show,\n",
    "                                         len(classifier.pose_class_names))\n",
    "\n",
    "  # Continuously capture images from the camera and run inference\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      sys.exit(\n",
    "          'ERROR: Unable to read from webcam. Please verify your webcam settings.'\n",
    "      )\n",
    "\n",
    "    counter += 1\n",
    "    image = cv2.flip(image, 1)\n",
    "\n",
    "    if estimation_model == 'movenet_multipose':\n",
    "      # Run pose estimation using a MultiPose model.\n",
    "      list_persons = pose_detector.detect(image)\n",
    "    else:\n",
    "      # Run pose estimation using a SinglePose model, and wrap the result in an\n",
    "      # array.\n",
    "      list_persons = [pose_detector.detect(image)]\n",
    "\n",
    "    # Draw keypoints and edges on input image\n",
    "    image = utils.visualize(image, list_persons)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Calculate the FPS\n",
    "    if counter % fps_avg_frame_count == 0:\n",
    "      end_time = time.time()\n",
    "      fps = fps_avg_frame_count / (end_time - start_time)\n",
    "      start_time = time.time()\n",
    "\n",
    "    # Show the FPS\n",
    "    fps_text = 'FPS = ' + str(int(fps))\n",
    "    text_location = (left_margin, row_size)\n",
    "    cv2.putText(image, fps_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
    "                font_size, text_color, font_thickness)\n",
    "\n",
    "    # Stop the program if the ESC key is pressed.\n",
    "    if cv2.waitKey(1) == 27:\n",
    "      break\n",
    "    cv2.imshow(estimation_model, image)\n",
    "\n",
    "  cap.release()\n",
    "  cv2.destroyAllWindows()\n",
    "    \n",
    "run('movenet_lightning', 'keypoint',None, None,\n",
    "      int(0), 800, 600)\n",
    "\n",
    "def main():\n",
    "  parser = argparse.ArgumentParser(\n",
    "      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "  parser.add_argument(\n",
    "      '--model',\n",
    "      help='Name of estimation model.',\n",
    "      required=False,\n",
    "      default='movenet_lightning')\n",
    "  parser.add_argument(\n",
    "      '--tracker',\n",
    "      help='Type of tracker to track poses across frames.',\n",
    "      required=False,\n",
    "      default='keypoint') #bounding_box,keypoint\n",
    "  parser.add_argument(\n",
    "      '--classifier', help='Name of classification model.', required=False)\n",
    "  parser.add_argument(\n",
    "      '--label_file',\n",
    "      help='Label file for classification.',\n",
    "      required=False,\n",
    "      default='labels.txt')\n",
    "  parser.add_argument(\n",
    "      '--cameraId', help='Id of camera.', required=False, default=0)\n",
    "  parser.add_argument(\n",
    "      '--frameWidth',\n",
    "      help='Width of frame to capture from camera.',\n",
    "      required=False,\n",
    "      default=800)\n",
    "  parser.add_argument(\n",
    "      '--frameHeight',\n",
    "      help='Height of frame to capture from camera.',\n",
    "      required=False,\n",
    "      default=600)\n",
    "  args = parser.parse_args()\n",
    "\n",
    "  run(args.model, args.tracker, args.classifier, args.label_file,\n",
    "      int(args.cameraId), args.frameWidth, args.frameHeight)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc401b58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
